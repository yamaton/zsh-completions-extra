#compdef fairseq-generate

# Auto-generated with h2o

function _fairseq-generate {
    _arguments \
        {-h,--help}'[show this help message and exit]' \
        '--no-progress-bar[disable progress bar]' \
        '--log-interval[log progress every N batches (when progress bar is disabled)]' \
        '--log-format[log format to use]' \
        '--log-file[log file to copy metrics to.]':file:_files \
        '--aim-repo[path to Aim repository]' \
        '--aim-run-hash[Aim run hash. If skipped, creates or continues run based on save_dir]' \
        '--tensorboard-logdir[path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)]':file:_files \
        '--wandb-project[Weights and Biases project name to use for logging]' \
        '--azureml-logging[Log scalars to AzureML context]' \
        '--seed[pseudo random number generator seed]' \
        '--cpu[use CPU instead of CUDA]' \
        '--tpu[use TPU instead of CUDA]' \
        '--bf16[use bfloat16; implies --tpu]' \
        '--memory-efficient-bf16[use a memory-efficient version of BF16 training; implies --bf16]' \
        '--fp16[use FP16]' \
        '--memory-efficient-fp16[use a memory-efficient version of FP16 training; implies --fp16]' \
        '--fp16-no-flatten-grads[don'\''t flatten FP16 grads tensor]' \
        '--fp16-init-scale[default FP16 loss scale]' \
        '--fp16-scale-window[number of updates before increasing loss scale]' \
        '--fp16-scale-tolerance[pct of updates that can overflow before decreasing the loss scale]' \
        '--on-cpu-convert-precision[if set, the floating point conversion to fp16/bf16 runs on CPU. This reduces bus transfer time and GPU memory usage.]' \
        '--min-loss-scale[minimum FP16/AMP loss scale, after which training is stopped]' \
        '--threshold-loss-scale[threshold FP16 loss scale from below]' \
        '--amp[use automatic mixed precision]' \
        '--amp-batch-retries[number of retries of same batch after reducing loss scale with AMP]' \
        '--amp-init-scale[default AMP loss scale]' \
        '--amp-scale-window[number of updates before increasing AMP loss scale]' \
        '--user-dir[path to a python module containing custom extensions (tasks and/or architectures)]':file:_files \
        '--empty-cache-freq[how often to clear the PyTorch CUDA cache (0 to disable)]' \
        '--all-gather-list-size[number of bytes reserved for gathering stats from workers]' \
        '--model-parallel-size[total number of GPUs to parallelize model over]' \
        '--quantization-config-path[path to quantization config file]':file:_files \
        '--profile[enable autograd profiler emit_nvtx]' \
        '--reset-logging[when using Hydra, reset the logging at the beginning of training]' \
        '--suppress-crashes[suppress crashes when training with the hydra_train entry point so that the main method can return a value (useful for sweeps)]' \
        '--use-plasma-view[Store indices and sizes in shared memory]' \
        '--plasma-path[path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend to fail.]':file:_files \
        {--criterion,--tokenizer,--bpe,--optimizer,--lr-scheduler,--scoring,--task}'[task]' \
        {--arch,-a}'[Model architecture. For constructing tasks that rely on model args (e.g. `AudioPretraining`)]' \
        '--num-workers[how many subprocesses to use for data loading]' \
        '--skip-invalid-size-inputs-valid-test[ignore too long or too short lines in valid and test set]' \
        '--max-tokens[maximum number of tokens in a batch]' \
        {--batch-size,--max-sentences}'[number of examples in a batch]' \
        '--required-batch-size-multiple[batch size will be a multiplier of this value]' \
        '--required-seq-len-multiple[maximum sequence length in batch will be a multiplier of this value]' \
        '--dataset-impl[output dataset implementation]' \
        '--data-buffer-size[Number of batches to preload]' \
        '--train-subset[data subset to use for training (e.g. train, valid, test)]' \
        '--valid-subset[comma separated list of data subsets to use for validation (e.g. train, valid, test)]' \
        {--combine-valid-subsets,--combine-val}'[comma separated list of data subsets to use for validation (e.g. train, valid, test)]' \
        '--ignore-unused-valid-subsets[do not raise error if valid subsets are ignored]' \
        '--validate-interval[validate every N epochs]' \
        '--validate-interval-updates[validate every N updates]' \
        '--validate-after-updates[dont validate until reaching this many updates]' \
        '--fixed-validation-seed[specified random seed for validation]' \
        '--disable-validation[disable validation]' \
        '--max-tokens-valid[maximum number of tokens in a validation batch (defaults to --max-tokens)]' \
        {--batch-size-valid,--max-sentences-valid}'[batch size of the validation batch (defaults to --batch-size)]' \
        {--max-valid-steps,--nval}'[How many batches to evaluate]' \
        '--curriculum[don'\''t shuffle batches for first N epochs]' \
        '--gen-subset[data subset to generate (train, valid, test)]' \
        '--num-shards[shard generation over N shards]' \
        '--shard-id[id of the shard to generate (id < num_shards)]' \
        '--grouped-shuffling[shuffle batches in groups of num_shards to enable similar sequence lengths on each GPU worker when batches are sorted by length]' \
        '--update-epoch-batch-itr[if true then prevents the reuse the epoch batch iterator by setting can_reuse_epoch_itr to false, defaults to --grouped-shuffling )]' \
        '--update-ordered-indices-seed[if true then increment seed with epoch for getting batch iterators, defautls to False.]' \
        '--distributed-world-size[total number of GPUs across all nodes (default: all visible GPUs)]' \
        '--distributed-num-procs[total number of processes to fork (default: all visible GPUs)]' \
        '--distributed-rank[rank of the current worker]' \
        '--distributed-backend[distributed backend]' \
        '--distributed-init-method[typically tcp://hostname:port that will be used to establish initial connetion]' \
        '--distributed-port[port number (not required if using --distributed-init-method)]' \
        {--device-id,--local_rank}'[which GPU to use (by default looks for $LOCAL_RANK, usually configured automatically)]' \
        '--distributed-no-spawn[do not spawn multiple processes even if multiple GPUs are visible]' \
        '--ddp-backend[DistributedDataParallel backend]' \
        '--ddp-comm-hook[communication hook]' \
        '--bucket-cap-mb[bucket size for reduction]' \
        '--fix-batches-to-gpus[don'\''t shuffle batches between GPUs; this reduces overall randomness and may affect precision but avoids the cost of re-reading the data]' \
        '--find-unused-parameters[disable unused parameter detection (not applicable to --ddp-backend=legacy_ddp)]' \
        '--gradient-as-bucket-view[when set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. --gradient-as-bucket-view=gradient_as_bucket_view)]' \
        '--fast-stat-sync[\[deprecated\] this is now defined per Criterion]' \
        '--heartbeat-timeout[kill the job if no progress is made in N seconds; set to -1 to disable]' \
        '--broadcast-buffers[Copy non-trainable parameters between GPUs, such as batchnorm population statistics]' \
        '--slowmo-momentum[SlowMo momentum term; by default use 0.0 for 16 GPUs, 0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs]' \
        '--slowmo-base-algorithm[Base algorithm. Either '\''localsgd'\'' or '\''sgp'\''. Please refer to the documentation of '\''slowmo_base_algorithm'\'' parameter in https://fairscale.readthedocs.io/en/latest/api/experimental/nn/slowmo_ddp.html for more details]' \
        '--localsgd-frequency[Local SGD allreduce frequency]' \
        '--nprocs-per-node[number of GPUs in each node. An allreduce operation across GPUs in a node is very fast. Hence, we do allreduce across GPUs in a node, and gossip across different nodes]' \
        '--pipeline-model-parallel[if set, use pipeline model parallelism across GPUs]' \
        '--pipeline-balance[partition the model into N_K pieces, where each piece contains N_i layers. The sum(args.pipeline_balance) should equal the total number of layers in the model]' \
        '--pipeline-devices[a list of device indices indicating which device to place each of the N_K partitions. The length of this list should equal the length of the --pipeline-balance argument]' \
        '--pipeline-chunks[microbatch count for pipeline model parallelism]' \
        '--pipeline-encoder-balance[partition the pipeline parallel encoder into N_K pieces, where each piece contains N_i layers. The sum(args.pipeline_encoder_balance) should equal the total number of encoder layers in the model]' \
        '--pipeline-encoder-devices[a list of device indices indicating which device to place each of the N_K partitions. The length of this list should equal the length of the --pipeline-encoder-balance argument]' \
        '--pipeline-decoder-balance[partition the pipeline parallel decoder into N_K pieces, where each piece contains N_i layers. The sum(args.pipeline_decoder_balance) should equal the total number of decoder layers in the model]' \
        '--pipeline-decoder-devices[a list of device indices indicating which device to place each of the N_K partitions. The length of this list should equal the length of the --pipeline-decoder-balance argument]' \
        '--pipeline-checkpoint[checkpointing mode for pipeline model parallelism]' \
        '--zero-sharding[ZeRO sharding]' \
        '--no-reshard-after-forward[don'\''t reshard parameters after forward pass]' \
        '--fp32-reduce-scatter[reduce-scatter grads in FP32]' \
        '--cpu-offload[offload FP32 params to CPU]' \
        '--use-sharded-state[use sharded checkpoint files]' \
        '--not-fsdp-flatten-parameters[not flatten parameter param for fsdp]' \
        '--path[path(s) to model file(s), colon separated]':file:_files \
        {--post-process,--remove-bpe}'[post-process text by removing BPE, letter segmentation, etc. Valid options can be found in fairseq.data.utils.post_process.]' \
        '--quiet[only print final scores]' \
        '--model-overrides[a dictionary used to override model args at generation that were used during model training]' \
        '--results-path[path to save eval results (optional)]':file:_files \
        '--beam[beam size]' \
        '--beam-mt[beam size for the first-pass decoder]' \
        '--nbest[number of hypotheses to output]' \
        '--max-len-a[generate sequences of maximum length ax + b, where x is the source length]' \
        '--max-len-b[generate sequences of maximum length ax + b, where x is the source length]' \
        '--max-len-a-mt[generate sequences of maximum length ax + b, where x is the source length for the first-pass decoder]' \
        '--max-len-b-mt[generate sequences of maximum length ax + b, where x is the source length for the first-pass decoder]' \
        '--min-len[minimum generation length]' \
        '--match-source-len[generations should match the source length]' \
        '--unnormalized[compare unnormalized hypothesis scores]' \
        '--no-early-stop[deprecated]' \
        '--no-beamable-mm[don'\''t use BeamableMM in attention layers]' \
        '--lenpen[length penalty: <1.0 favors shorter, >1.0 favors longer sentences]' \
        '--lenpen-mt[length penalty for the first-pass decoder: <1.0 favors shorter, >1.0 favors longer sentences]' \
        '--unkpen[unknown word penalty: <0 produces more unks, >0 produces fewer]' \
        '--replace-unk[perform unknown replacement (optionally with alignment dictionary)]' \
        '--sacrebleu[score with sacrebleu]' \
        '--score-reference[just score the reference translation]' \
        '--prefix-size[initialize generation by target prefix of given length]' \
        '--no-repeat-ngram-size[ngram blocking such that this size ngram cannot be repeated in the generation]' \
        '--sampling[sample hypotheses instead of using beam search]' \
        '--sampling-topk[sample from top K likely next words instead of all words]' \
        '--sampling-topp[sample from the smallest set whose cumulative probability mass exceeds p for next words]' \
        '--constraints[enables lexically constrained decoding]' \
        '--temperature[temperature for generation]' \
        '--diverse-beam-groups[number of groups for Diverse Beam Search]' \
        '--diverse-beam-strength[strength of diversity penalty for Diverse Beam Search]' \
        '--diversity-rate[strength of diversity penalty for Diverse Siblings Search]' \
        '--print-alignment[if set, uses attention feedback to compute and print alignment to source tokens (valid options are: hard, soft, otherwise treated as hard alignment)]' \
        '--print-step[print steps]' \
        '--lm-path[path to lm checkpoint for lm fusion]':file:_files \
        '--lm-weight[weight for lm probs for lm fusion]' \
        '--iter-decode-eos-penalty[if > 0.0, it penalized early-stopping in decoding.]' \
        '--iter-decode-max-iter[maximum iterations for iterative refinement.]' \
        '--iter-decode-force-max-iter[if set, run exact the maximum number of iterations without early stop]' \
        '--iter-decode-with-beam[if > 1, model will generate translations varying by the lengths.]' \
        '--iter-decode-with-external-reranker[if set, the last checkpoint are assumed to be a reranker to rescore the translations]' \
        '--retain-iter-history[if set, decoding returns the whole history of iterative refinement]' \
        '--retain-dropout[Use dropout at inference time]' \
        '--retain-dropout-modules[if set, only retain dropout for the specified modules; if not set, then dropout will be retained for all modules]' \
        '--decoding-format[special decoding format for advanced decoding.]' \
        '--no-seed-provided[if set, dont use seed for initializing random generators]' \
        '--eos-token[EOS token]' \
        '--save-dir[path to save checkpoints]':file:_files \
        '--restore-file[filename from which to load checkpoint (default: <save-dir>/checkpoint_last.pt]':file:_files \
        '--continue-once[continues from this checkpoint, unless a checkpoint indicated in '\''restore_file'\'' option is present]' \
        '--finetune-from-model[finetune from a pretrained model; note that meters and lr scheduler will be reset]' \
        '--reset-dataloader[if set, does not reload dataloader state from the checkpoint]' \
        '--reset-lr-scheduler[if set, does not load lr scheduler state from the checkpoint]' \
        '--reset-meters[if set, does not load meters from the checkpoint]' \
        '--reset-optimizer[if set, does not load optimizer state from the checkpoint]' \
        '--optimizer-overrides[a dictionary used to override optimizer args when loading a checkpoint]' \
        '--save-interval[save a checkpoint every N epochs]' \
        '--save-interval-updates[save a checkpoint (and validate) every N updates]' \
        '--keep-interval-updates[keep the last N checkpoints saved with --save-interval-updates]' \
        '--keep-interval-updates-pattern[when used with --keep-interval-updates, skips deleting any checkpoints with update X where X % keep_interval_updates_pattern == 0]' \
        '--keep-last-epochs[keep last N epoch checkpoints]' \
        '--keep-best-checkpoints[keep best N checkpoints based on scores]' \
        '--no-save[don'\''t save models or checkpoints]' \
        '--no-epoch-checkpoints[only store last and best checkpoints]' \
        '--no-last-checkpoints[don'\''t store last checkpoints]' \
        '--no-save-optimizer-state[don'\''t save optimizer-state as part of checkpoint]' \
        '--best-checkpoint-metric[metric to use for saving "best" checkpoints]' \
        '--maximize-best-checkpoint-metric[select the largest metric value for saving "best" checkpoints]' \
        '--patience[early stop training if valid performance doesn'\''t improve for N consecutive validation runs; note that this is influenced by --validate-interval]' \
        '--checkpoint-suffix[suffix to add to the checkpoint file name]' \
        '--checkpoint-shard-count[Number of shards containing the checkpoint - if the checkpoint is over 300GB, it is preferable to split it into shards to prevent OOM on CPU while loading the checkpoint]' \
        '--load-checkpoint-on-all-dp-ranks[load checkpoints on all data parallel devices (default: only load on rank 0 and broadcast to other devices)]' \
        {--write-checkpoints-asynchronously,--save-async}'[Write checkpoints asynchronously in a separate thread. NOTE: This feature is currently being tested.]' \
        "*: :_files"

}

_fairseq-generate "$@"

