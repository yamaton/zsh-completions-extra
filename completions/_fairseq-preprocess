#compdef fairseq-preprocess

# Auto-generated with h2o

function _fairseq-preprocess {
    _arguments \
        {-h,--help}'[show this help message and exit]' \
        '--no-progress-bar[disable progress bar]' \
        '--log-interval[log progress every N batches (when progress bar is disabled)]' \
        '--log-format[log format to use]' \
        '--log-file[log file to copy metrics to.]':file:_files \
        '--aim-repo[path to Aim repository]' \
        '--aim-run-hash[Aim run hash. If skipped, creates or continues run based on save_dir]' \
        '--tensorboard-logdir[path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)]':file:_files \
        '--wandb-project[Weights and Biases project name to use for logging]' \
        '--azureml-logging[Log scalars to AzureML context]' \
        '--seed[pseudo random number generator seed]' \
        '--cpu[use CPU instead of CUDA]' \
        '--tpu[use TPU instead of CUDA]' \
        '--bf16[use bfloat16; implies --tpu]' \
        '--memory-efficient-bf16[use a memory-efficient version of BF16 training; implies --bf16]' \
        '--fp16[use FP16]' \
        '--memory-efficient-fp16[use a memory-efficient version of FP16 training; implies --fp16]' \
        '--fp16-no-flatten-grads[don'\''t flatten FP16 grads tensor]' \
        '--fp16-init-scale[default FP16 loss scale]' \
        '--fp16-scale-window[number of updates before increasing loss scale]' \
        '--fp16-scale-tolerance[pct of updates that can overflow before decreasing the loss scale]' \
        '--on-cpu-convert-precision[if set, the floating point conversion to fp16/bf16 runs on CPU. This reduces bus transfer time and GPU memory usage.]' \
        '--min-loss-scale[minimum FP16/AMP loss scale, after which training is stopped]' \
        '--threshold-loss-scale[threshold FP16 loss scale from below]' \
        '--amp[use automatic mixed precision]' \
        '--amp-batch-retries[number of retries of same batch after reducing loss scale with AMP]' \
        '--amp-init-scale[default AMP loss scale]' \
        '--amp-scale-window[number of updates before increasing AMP loss scale]' \
        '--user-dir[path to a python module containing custom extensions (tasks and/or architectures)]':file:_files \
        '--empty-cache-freq[how often to clear the PyTorch CUDA cache (0 to disable)]' \
        '--all-gather-list-size[number of bytes reserved for gathering stats from workers]' \
        '--model-parallel-size[total number of GPUs to parallelize model over]' \
        '--quantization-config-path[path to quantization config file]':file:_files \
        '--profile[enable autograd profiler emit_nvtx]' \
        '--reset-logging[when using Hydra, reset the logging at the beginning of training]' \
        '--suppress-crashes[suppress crashes when training with the hydra_train entry point so that the main method can return a value (useful for sweeps)]' \
        '--use-plasma-view[Store indices and sizes in shared memory]' \
        '--plasma-path[path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend to fail.]':file:_files \
        {--criterion,--tokenizer,--bpe,--optimizer,--lr-scheduler,--scoring,--task}'[task]' \
        '--dataset-impl[output dataset implementation]' \
        {-s,--source-lang}'[source language]' \
        {-t,--target-lang}'[target language]' \
        '--trainpref[train file prefix (also used to build dictionaries)]' \
        '--validpref[comma separated, valid file prefixes (words missing from train set are replaced with <unk>)]' \
        '--testpref[comma separated, test file prefixes (words missing from train set are replaced with <unk>)]' \
        '--align-suffix[alignment file suffix]' \
        '--destdir[destination dir]':file:_files \
        '--thresholdtgt[map words appearing less than threshold times to unknown]' \
        '--thresholdsrc[map words appearing less than threshold times to unknown]' \
        '--tgtdict[reuse given target dictionary]' \
        '--srcdict[reuse given source dictionary]' \
        '--nwordstgt[number of target words to retain]' \
        '--nwordssrc[number of source words to retain]' \
        '--alignfile[an alignment file (optional)]' \
        '--joined-dictionary[Generate joined dictionary]' \
        '--only-source[Only process the source language]' \
        '--padding-factor[Pad dictionary size to be multiple of N]' \
        '--workers[number of parallel workers]' \
        '--dict-only[if true, only builds a dictionary and then exits]' \
        "*: :_files"

}

_fairseq-preprocess "$@"

