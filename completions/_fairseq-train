#compdef fairseq-train

# Auto-generated with h2o

function _fairseq-train {
    _arguments \
        {-h,--help}'[show this help message and exit]' \
        '--no-progress-bar[disable progress bar]' \
        '--log-interval[log progress every N batches (when progress bar is disabled)]' \
        '--log-format[log format to use]' \
        '--log-file[log file to copy metrics to.]':file:_files \
        '--aim-repo[path to Aim repository]' \
        '--aim-run-hash[Aim run hash. If skipped, creates or continues run based on save_dir]' \
        '--tensorboard-logdir[path to save logs for tensorboard, should match --logdir of running tensorboard (default: no tensorboard logging)]':file:_files \
        '--wandb-project[Weights and Biases project name to use for logging]' \
        '--azureml-logging[Log scalars to AzureML context]' \
        '--seed[pseudo random number generator seed]' \
        '--cpu[use CPU instead of CUDA]' \
        '--tpu[use TPU instead of CUDA]' \
        '--bf16[use bfloat16; implies --tpu]' \
        '--memory-efficient-bf16[use a memory-efficient version of BF16 training; implies --bf16]' \
        '--fp16[use FP16]' \
        '--memory-efficient-fp16[use a memory-efficient version of FP16 training; implies --fp16]' \
        '--fp16-no-flatten-grads[don'\''t flatten FP16 grads tensor]' \
        '--fp16-init-scale[default FP16 loss scale]' \
        '--fp16-scale-window[number of updates before increasing loss scale]' \
        '--fp16-scale-tolerance[pct of updates that can overflow before decreasing the loss scale]' \
        '--on-cpu-convert-precision[if set, the floating point conversion to fp16/bf16 runs on CPU. This reduces bus transfer time and GPU memory usage.]' \
        '--min-loss-scale[minimum FP16/AMP loss scale, after which training is stopped]' \
        '--threshold-loss-scale[threshold FP16 loss scale from below]' \
        '--amp[use automatic mixed precision]' \
        '--amp-batch-retries[number of retries of same batch after reducing loss scale with AMP]' \
        '--amp-init-scale[default AMP loss scale]' \
        '--amp-scale-window[number of updates before increasing AMP loss scale]' \
        '--user-dir[path to a python module containing custom extensions (tasks and/or architectures)]':file:_files \
        '--empty-cache-freq[how often to clear the PyTorch CUDA cache (0 to disable)]' \
        '--all-gather-list-size[number of bytes reserved for gathering stats from workers]' \
        '--model-parallel-size[total number of GPUs to parallelize model over]' \
        '--quantization-config-path[path to quantization config file]':file:_files \
        '--profile[enable autograd profiler emit_nvtx]' \
        '--reset-logging[when using Hydra, reset the logging at the beginning of training]' \
        '--suppress-crashes[suppress crashes when training with the hydra_train entry point so that the main method can return a value (useful for sweeps)]' \
        '--use-plasma-view[Store indices and sizes in shared memory]' \
        '--plasma-path[path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend to fail.]':file:_files \
        {--criterion,--tokenizer,--bpe,--optimizer,--lr-scheduler,--scoring,--task}'[task]' \
        '--num-workers[how many subprocesses to use for data loading]' \
        '--skip-invalid-size-inputs-valid-test[ignore too long or too short lines in valid and test set]' \
        '--max-tokens[maximum number of tokens in a batch]' \
        {--batch-size,--max-sentences}'[number of examples in a batch]' \
        '--required-batch-size-multiple[batch size will be a multiplier of this value]' \
        '--required-seq-len-multiple[maximum sequence length in batch will be a multiplier of this value]' \
        '--dataset-impl[output dataset implementation]' \
        '--data-buffer-size[Number of batches to preload]' \
        '--train-subset[data subset to use for training (e.g. train, valid, test)]' \
        '--valid-subset[comma separated list of data subsets to use for validation (e.g. train, valid, test)]' \
        {--combine-valid-subsets,--combine-val}'[comma separated list of data subsets to use for validation (e.g. train, valid, test)]' \
        '--ignore-unused-valid-subsets[do not raise error if valid subsets are ignored]' \
        '--validate-interval[validate every N epochs]' \
        '--validate-interval-updates[validate every N updates]' \
        '--validate-after-updates[dont validate until reaching this many updates]' \
        '--fixed-validation-seed[specified random seed for validation]' \
        '--disable-validation[disable validation]' \
        '--max-tokens-valid[maximum number of tokens in a validation batch (defaults to --max-tokens)]' \
        {--batch-size-valid,--max-sentences-valid}'[batch size of the validation batch (defaults to --batch-size)]' \
        {--max-valid-steps,--nval}'[How many batches to evaluate]' \
        '--curriculum[don'\''t shuffle batches for first N epochs]' \
        '--gen-subset[data subset to generate (train, valid, test)]' \
        '--num-shards[shard generation over N shards]' \
        '--shard-id[id of the shard to generate (id < num_shards)]' \
        '--grouped-shuffling[shuffle batches in groups of num_shards to enable similar sequence lengths on each GPU worker when batches are sorted by length]' \
        '--update-epoch-batch-itr[if true then prevents the reuse the epoch batch iterator by setting can_reuse_epoch_itr to false, defaults to --grouped-shuffling )]' \
        '--update-ordered-indices-seed[if true then increment seed with epoch for getting batch iterators, defautls to False.]' \
        '--distributed-world-size[total number of GPUs across all nodes (default: all visible GPUs)]' \
        '--distributed-num-procs[total number of processes to fork (default: all visible GPUs)]' \
        '--distributed-rank[rank of the current worker]' \
        '--distributed-backend[distributed backend]' \
        '--distributed-init-method[typically tcp://hostname:port that will be used to establish initial connetion]' \
        '--distributed-port[port number (not required if using --distributed-init-method)]' \
        {--device-id,--local_rank}'[which GPU to use (by default looks for $LOCAL_RANK, usually configured automatically)]' \
        '--distributed-no-spawn[do not spawn multiple processes even if multiple GPUs are visible]' \
        '--ddp-backend[DistributedDataParallel backend]' \
        '--ddp-comm-hook[communication hook]' \
        '--bucket-cap-mb[bucket size for reduction]' \
        '--fix-batches-to-gpus[don'\''t shuffle batches between GPUs; this reduces overall randomness and may affect precision but avoids the cost of re-reading the data]' \
        '--find-unused-parameters[disable unused parameter detection (not applicable to --ddp-backend=legacy_ddp)]' \
        '--gradient-as-bucket-view[when set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. --gradient-as-bucket-view=gradient_as_bucket_view)]' \
        '--fast-stat-sync[\[deprecated\] this is now defined per Criterion]' \
        '--heartbeat-timeout[kill the job if no progress is made in N seconds; set to -1 to disable]' \
        '--broadcast-buffers[Copy non-trainable parameters between GPUs, such as batchnorm population statistics]' \
        '--slowmo-momentum[SlowMo momentum term; by default use 0.0 for 16 GPUs, 0.2 for 32 GPUs; 0.5 for 64 GPUs, 0.6 for > 64 GPUs]' \
        '--slowmo-base-algorithm[Base algorithm. Either '\''localsgd'\'' or '\''sgp'\''. Please refer to the documentation of '\''slowmo_base_algorithm'\'' parameter in https://fairscale.readthedocs.io/en/latest/api/experimental/nn/slowmo_ddp.html for more details]' \
        '--localsgd-frequency[Local SGD allreduce frequency]' \
        '--nprocs-per-node[number of GPUs in each node. An allreduce operation across GPUs in a node is very fast. Hence, we do allreduce across GPUs in a node, and gossip across different nodes]' \
        '--pipeline-model-parallel[if set, use pipeline model parallelism across GPUs]' \
        '--pipeline-balance[partition the model into N_K pieces, where each piece contains N_i layers. The sum(args.pipeline_balance) should equal the total number of layers in the model]' \
        '--pipeline-devices[a list of device indices indicating which device to place each of the N_K partitions. The length of this list should equal the length of the --pipeline-balance argument]' \
        '--pipeline-chunks[microbatch count for pipeline model parallelism]' \
        '--pipeline-encoder-balance[partition the pipeline parallel encoder into N_K pieces, where each piece contains N_i layers. The sum(args.pipeline_encoder_balance) should equal the total number of encoder layers in the model]' \
        '--pipeline-encoder-devices[a list of device indices indicating which device to place each of the N_K partitions. The length of this list should equal the length of the --pipeline-encoder-balance argument]' \
        '--pipeline-decoder-balance[partition the pipeline parallel decoder into N_K pieces, where each piece contains N_i layers. The sum(args.pipeline_decoder_balance) should equal the total number of decoder layers in the model]' \
        '--pipeline-decoder-devices[a list of device indices indicating which device to place each of the N_K partitions. The length of this list should equal the length of the --pipeline-decoder-balance argument]' \
        '--pipeline-checkpoint[checkpointing mode for pipeline model parallelism]' \
        '--zero-sharding[ZeRO sharding]' \
        '--no-reshard-after-forward[don'\''t reshard parameters after forward pass]' \
        '--fp32-reduce-scatter[reduce-scatter grads in FP32]' \
        '--cpu-offload[offload FP32 params to CPU]' \
        '--use-sharded-state[use sharded checkpoint files]' \
        '--not-fsdp-flatten-parameters[not flatten parameter param for fsdp]' \
        {--arch,-a}'[model architecture]' \
        '--max-epoch[force stop training at specified epoch]' \
        '--max-update[force stop training at specified update]' \
        '--stop-time-hours[force stop training after specified cumulative time (if >0)]' \
        '--clip-norm[clip threshold of gradients]' \
        '--sentence-avg[normalize gradients by the number of sentences in a batch (default is to normalize by number of tokens)]' \
        '--update-freq[update parameters every N_i batches, when in epoch i]' \
        '--lr[learning rate for the first N epochs; all epochs >N using LR_N (note: this may be interpreted differently depending on --lr-scheduler)]' \
        '--stop-min-lr[stop training when the learning rate reaches this minimum]' \
        '--use-bmuf[specify global optimizer for syncing models on different GPUs/shards]' \
        '--skip-remainder-batch[if set, include the last (partial) batch of each epoch in training (default is to skip it).]' \
        '--save-dir[path to save checkpoints]':file:_files \
        '--restore-file[filename from which to load checkpoint (default: <save-dir>/checkpoint_last.pt]':file:_files \
        '--continue-once[continues from this checkpoint, unless a checkpoint indicated in '\''restore_file'\'' option is present]' \
        '--finetune-from-model[finetune from a pretrained model; note that meters and lr scheduler will be reset]' \
        '--reset-dataloader[if set, does not reload dataloader state from the checkpoint]' \
        '--reset-lr-scheduler[if set, does not load lr scheduler state from the checkpoint]' \
        '--reset-meters[if set, does not load meters from the checkpoint]' \
        '--reset-optimizer[if set, does not load optimizer state from the checkpoint]' \
        '--optimizer-overrides[a dictionary used to override optimizer args when loading a checkpoint]' \
        '--save-interval[save a checkpoint every N epochs]' \
        '--save-interval-updates[save a checkpoint (and validate) every N updates]' \
        '--keep-interval-updates[keep the last N checkpoints saved with --save-interval-updates]' \
        '--keep-interval-updates-pattern[when used with --keep-interval-updates, skips deleting any checkpoints with update X where X % keep_interval_updates_pattern == 0]' \
        '--keep-last-epochs[keep last N epoch checkpoints]' \
        '--keep-best-checkpoints[keep best N checkpoints based on scores]' \
        '--no-save[don'\''t save models or checkpoints]' \
        '--no-epoch-checkpoints[only store last and best checkpoints]' \
        '--no-last-checkpoints[don'\''t store last checkpoints]' \
        '--no-save-optimizer-state[don'\''t save optimizer-state as part of checkpoint]' \
        '--best-checkpoint-metric[metric to use for saving "best" checkpoints]' \
        '--maximize-best-checkpoint-metric[select the largest metric value for saving "best" checkpoints]' \
        '--patience[early stop training if valid performance doesn'\''t improve for N consecutive validation runs; note that this is influenced by --validate-interval]' \
        '--checkpoint-suffix[suffix to add to the checkpoint file name]' \
        '--checkpoint-shard-count[Number of shards containing the checkpoint - if the checkpoint is over 300GB, it is preferable to split it into shards to prevent OOM on CPU while loading the checkpoint]' \
        '--load-checkpoint-on-all-dp-ranks[load checkpoints on all data parallel devices (default: only load on rank 0 and broadcast to other devices)]' \
        {--write-checkpoints-asynchronously,--save-async}'[Write checkpoints asynchronously in a separate thread. NOTE: This feature is currently being tested.]' \
        {--store-ema,--ema-decay}'[decay for exponential moving average model]' \
        '--ema-start-update[start EMA update after this many model updates]' \
        '--ema-seed-model[Seed to load EMA model from. Used to load EMA model separately from the actual model.]' \
        '--ema-update-freq[Do EMA update every this many model updates]' \
        '--ema-fp32[If true, store EMA model in fp32 even if model is in fp16]' \
        "*: :_files"

}

_fairseq-train "$@"

